---
title: "Motivation"
author: "Dr. Simon MÃ¼ller"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    toc_float: true
    theme: readable
    highlight: tango
    css: kable.css
vignette: >
  %\VignetteIndexEntry{Motivation}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Objective

### Define a Distance Measure for Numerical and Categorical Features

Let $x_i = \left(x_i^1,\cdots,x_i^p\right)^T$ and $x_j = \left(x_j^1,\cdots,x_j^p\right)^T$
be the feature vectors of tow distinct patients $i$ and $j$. A first rough idea may be to calculate the $L_1-$norm of this two feature vectors:
$$
\|x_i - x_j\|_{L_1}=\frac{1}{p}\sum\limits_{k=1}^p|x_i^k - x_j^k|
$$

But this naive approach arise some problems:

1. This is well defined for numerical features, but what's about categorical features, e.g. gender (male/female). How shall we define the distance here?

2. $|x_i^k - x_j^k|$ is not scale invariant, e.g. if you change from meters to centimetres, the contribution of this feature would explode by a factor of 100. Features with large values will dominate the distance. 

3. All features are treated the same way, e.g. lung cancer: smoker (no/yes) seems to be more important than the city the patient is living.

We propose a machine learning approach for overcoming this issues.

## Weighted Distance Measure

Two steps have to be done to overcome above problems:
1. Generalize the L1-distance on the feature scale, such that it can deal with different variable types
2. Each feature needs to be weighted suitable 
  a. to rule out dependency on the scale
  b. account for varying importance across features
  c. impact size of each feature

This leads to following weighted distance measure:

$$
d(x_i, x_j) = \sum\limits_{k=1}^p |\alpha(x_i^k, x_j^k) d(x_i^k, x_j^k)|
$$

# Statistical Model

Let $\alpha(x_i^k, x_j^k)$ the weighting and $d(x_i^k, x_j^k)$ the distance for feature $k$. They are defined as follows:

If feature $k$ is numerical, then

$$d(x_i^k, x_j^k) = x_i^k - x_j^k$$ and

$$\alpha(x_i^k, x_j^k) = \frac{\hat{\beta_k}}{\sum \hat{\beta_k}}$$

If feature $k$ is categorical, then we define

$$d(x_i^k, x_j^k) = 1 \text{ when } x_i^k = x_j^k \text{ else } 0$$ and
$$\alpha(x_i^k, x_j^k) = \frac{\hat{\beta_k}^i - \hat{\beta_k}^j}{\sum \hat{\beta_k}}.$$
Where the $\hat{\beta_k}^k$ are coefficients of a regression model (linear, logistic, or PH model).

This approach solves all three problems described above. 


# Random Survival Forests

tt





